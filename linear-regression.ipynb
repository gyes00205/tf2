{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(9.0, shape=(), dtype=float32) tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y,x)\n",
    "print(y,y_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(125.0, shape=(), dtype=float32) tf.Tensor(\n",
      "[[ 70.]\n",
      " [100.]], shape=(2, 1), dtype=float32) tf.Tensor(30.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = tf.reduce_sum(tf.square(tf.matmul(X,w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 計算L(w, b)關於w, b的偏導數\n",
    "print(L, w_grad, b_grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 考慮一個實際問題，某城市在 2013 年 - 2017 年的房價如下表所示：\n",
    "\n",
    "## 年份 2013   2014   2015   2016   2017\n",
    "\n",
    "## 房價 12000 14000 15000 16500 17500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ]\n",
      "[0.         0.36363637 0.54545456 0.8181818  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy  as np\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "\n",
    "# 正規化處理\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy下的線性回歸\n",
    "機器學習模型的實現並不是 TensorFlow 的專利。事實上，對於簡單的模型，即使使用常規的科學計算套件或者工具也可以求解。在這裡，我們使用 NumPy 這一通用的科學計算套件來實現梯度下降方法。NumPy 提供了多維陣列支援，可以表示向量、矩陣以及更高維的張量。同時，也提供了大量支援在多維陣列上進行操作的函數（比如下面的 np.dot() 是求內積， np.sum() 是求和）。在這方面，NumPy 和 MATLAB 比較類似。在以下程式碼中，我們推導求損失函數關於參數 a 和 b 的偏導數，並使用梯度下降法反複疊代，最終獲得 a 和 b 的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763702027872221 0.057564988311377796\n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 5e-4\n",
    "for e in range(num_epoch):\n",
    "    # 手動計算損失函數關於自變數（模型參數）的梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = 2 * (y_pred - y).dot(X), 2 * (y_pred - y).sum()\n",
    "\n",
    "    # 更新參數\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 下的線性回歸\n",
    "TensorFlow 的 即時執行模式 與上述 NumPy 的運行方式十分類似，然而提供了更快速的運算（GPU 支援）、自動推導、優化器等一系列對深度學習非常重要的功能。以下展示了如何使用 TensorFlow 計算線性回歸。可以注意到，程式的結構和前述 NumPy 的實現非常類似。這裡，TensorFlow 幫助我們做了兩件重要的工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.9757488> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057909347>\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=1.)\n",
    "b = tf.Variable(initial_value=2.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-4)\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()記錄損失函數的梯度資訊\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a*X + b\n",
    "        loss = tf.reduce_sum(tf.square(y_pred-y))\n",
    "    # TensorFlow自動計算損失函數關於自變數（模型參數）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "    \n",
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
